{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Boston Housing Dataset\n",
    "\n",
    "#### The Boston Housing Dataset is a derived from information collected by the U.S. Census Service concerning housing in the area of Boston MA. The objective is to predict the value of prices of the house using the given features.The following describes the dataset columns:\n",
    "\n",
    "CRIM - per capita crime rate by town\n",
    "<br>ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "<br>INDUS - proportion of non-retail business acres per town.\n",
    "<br>CHAS - Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "<br>NOX - nitric oxides concentration (parts per 10 million)\n",
    "<br>RM - average number of rooms per dwelling\n",
    "<br>AGE - proportion of owner-occupied units built prior to 1940\n",
    "<br>DIS - weighted distances to five Boston employment centres\n",
    "<br>RAD - index of accessibility to radial highways\n",
    "<br>TAX - full-value property-tax rate per 10,000 USD\n",
    "<br>PTRATIO - pupil-teacher ratio by town\n",
    "<br>B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "<br>LSTAT - % lower status of the population\n",
    "<br>MEDV - Median value of owner-occupied homes in thousdands\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import pandas as pd  \n",
    "import seaborn as sns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing DataSet from scikit-learn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "# Any results you write to the current directory are saved as output.\n",
    "from pandas import read_csv\n",
    "#Lets load the dataset and sample some\n",
    "column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n",
    "data = read_csv('../input/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n",
    "print(data.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension of the dataset\n",
    "print(np.shape(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's summarize the data to see the distribution of data\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From get-go, two data coulmns show interesting summeries. They are : ZN (proportion of residential land zoned for lots over 25,000 sq.ft.) with 0 for 25th, 50th percentiles. Second, CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise) with 0 for 25th, 50th and 75th percentiles. These summeries are understandable as both variables are conditional + categorical variables. First assumption would be that these coulms may not be useful in regression task such as predicting MEDV (Median value of owner-occupied homes).\n",
    "\n",
    "Another interesing fact on the dataset is the max value of MEDV. From the original data description, it says: Variable #14 seems to be censored at 50.00 (corresponding to a median price of $50,000). Based on that, values above 50.00 may not help to predict MEDV. Let's plot the dataset and see interesting trends/stats.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k,v in data.items():\n",
    "    sns.boxplot(y=k, data=data, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns like CRIM, ZN, RM, B seems to have outliers. Let's see the outliers percentage in every column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    for k, v in data.items():\n",
    "        q1 = v.quantile(0.25)\n",
    "        q3 = v.quantile(0.75)\n",
    "        irq = q3 - q1\n",
    "        v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n",
    "        perc = np.shape(v_col)[0] * 100.0 / np.shape(data)[0]\n",
    "        print(\"Column %s outliers = %.2f%%\" % (k, perc))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove MEDV outliers (MEDV = 50.0) before plotting more distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[~(data['MEDV'] >= 50.0)]\n",
    "print(np.shape(data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how these features plus MEDV distributions looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=7, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for k,v in data.items():\n",
    "    sns.distplot(v, ax=axs[index])\n",
    "    index += 1\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram also shows that columns CRIM, ZN, B has highly skewed distributions. Also MEDV looks to have a normal distribution (the predictions) and other colums seem to have norma or bimodel ditribution of data except CHAS (which is a discrete variable).\n",
    "\n",
    "Now let's plot the pairwise correlation on data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(data.corr().abs(),  annot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From correlation matrix, we see TAX and RAD are highly correlated features. The columns LSTAT, INDUS, RM, TAX, NOX, PTRAIO has a correlation score above 0.5 with MEDV which is a good indication of using as predictors. Let's plot these columns against MEDV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# Let's scale the columns before plotting them against MEDV\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "column_sels = ['LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE']\n",
    "x = data.loc[:,column_sels]\n",
    "y = data['MEDV']\n",
    "x = pd.DataFrame(data=min_max_scaler.fit_transform(x), columns=column_sels)\n",
    "fig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20, 10))\n",
    "index = 0\n",
    "axs = axs.flatten()\n",
    "for i, k in enumerate(column_sels):\n",
    "    sns.regplot(y=y, x=x[k], ax=axs[i])\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So with these analsis, we may try predict MEDV with 'LSTAT', 'INDUS', 'NOX', 'PTRATIO', 'RM', 'TAX', 'DIS', 'AGE' features. Let's try to remove the skewness of the data trough log transformation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y =  np.log1p(y)\n",
    "for col in x.columns:\n",
    "    if np.abs(x[col].skew()) > 0.3:\n",
    "        x[col] = np.log1p(x[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try Linear, Ridge Regression on dataset first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "l_regression = linear_model.LinearRegression()\n",
    "kf = KFold(n_splits=10)\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "scores = cross_val_score(l_regression, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "\n",
    "scores_map = {}\n",
    "scores_map['LinearRegression'] = scores\n",
    "l_ridge = linear_model.Ridge()\n",
    "scores = cross_val_score(l_ridge, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['Ridge'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "\n",
    "# Lets try polinomial regression with L2 with degree for the best fit\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "#for degree in range(2, 6):\n",
    "#    model = make_pipeline(PolynomialFeatures(degree=degree), linear_model.Ridge())\n",
    "#    scores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "#    print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "model = make_pipeline(PolynomialFeatures(degree=3), linear_model.Ridge())\n",
    "scores = cross_val_score(model, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['PolyRidge'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Liner Regression with and without L2 regularization does not make significant difference is MSE score. However polynomial regression with degree=3 has a better MSE. Let's try some non prametric regression techniques: SVR with kernal rbf, DecisionTreeRegressor, KNeighborsRegressor etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\n",
    "#grid_sv = GridSearchCV(svr_rbf, cv=kf, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\n",
    "#grid_sv.fit(x_scaled, y)\n",
    "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
    "scores = cross_val_score(svr_rbf, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['SVR'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "desc_tr = DecisionTreeRegressor(max_depth=5)\n",
    "#grid_sv = GridSearchCV(desc_tr, cv=kf, param_grid={\"max_depth\" : [1, 2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n",
    "#grid_sv.fit(x_scaled, y)\n",
    "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
    "scores = cross_val_score(desc_tr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['DecisionTreeRegressor'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn = KNeighborsRegressor(n_neighbors=7)\n",
    "scores = cross_val_score(knn, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['KNeighborsRegressor'] = scores\n",
    "#grid_sv = GridSearchCV(knn, cv=kf, param_grid={\"n_neighbors\" : [2, 3, 4, 5, 6, 7]}, scoring='neg_mean_squared_error')\n",
    "#grid_sv.fit(x_scaled, y)\n",
    "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
    "print(\"KNN Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to three models which are shosen through grid search, SVR performes better. Let's try an ensemble method finally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(alpha=0.9,learning_rate=0.05, max_depth=2, min_samples_leaf=5, min_samples_split=2, n_estimators=100, random_state=30)\n",
    "#param_grid={'n_estimators':[100, 200], 'learning_rate': [0.1,0.05,0.02], 'max_depth':[2, 4,6], 'min_samples_leaf':[3,5,9]}\n",
    "#grid_sv = GridSearchCV(gbr, cv=kf, param_grid=param_grid, scoring='neg_mean_squared_error')\n",
    "#grid_sv.fit(x_scaled, y)\n",
    "#print(\"Best classifier :\", grid_sv.best_estimator_)\n",
    "scores = cross_val_score(gbr, x_scaled, y, cv=kf, scoring='neg_mean_squared_error')\n",
    "scores_map['GradientBoostingRegressor'] = scores\n",
    "print(\"MSE: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot k-fold results to see which model has better distribution of results. Let's have a look at the MSE distribution of these models with k-fold=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "scores_map = pd.DataFrame(scores_map)\n",
    "sns.boxplot(data=scores_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models SVR and GradientBoostingRegressor show better performance with -11.62 (+/- 5.91) and -12.39 (+/- 5.86).\n",
    "\n",
    "This is my first kernel and thanks to https://www.kaggle.com/vikrishnan for the dataset and the well writtten kernel that provdies great pointers into this dataset.\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
